# -*- coding: utf-8 -*-
"""scene-recog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16IY3Q3k616LiFGdJc4C7m4H_oWcF1kCD
"""

!nvidia-smi          # confirm T4 GPU is active
!pip install -q timm==0.9.2 scikit-learn matplotlib

import os, glob, shutil, random, time, copy
import numpy as np
import torch, timm
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# 🔧 CONFIG
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DATA_SRC = "/content/dataset/seg_train"
# change if different
DATA_DIR = '/content/dataset'            # where we'll create train/val/test
IMG_SIZE = 224
BATCH = 32
SEED = 42
print("Using device:", DEVICE)

# ✅ Safe dataset-split helper (handles nested seg_train and skips if already split)
import os, glob, random, shutil
from sklearn.model_selection import train_test_split

# ---------- CONFIG ----------
SEED = 42
UNZIP_ROOT = '/content/dataset'           # folder containing seg_train / seg_test
DATA_DIR = '/content/dataset_split'       # new folder for train/val/test splits
IMG_EXTS = ('.jpg', '.jpeg', '.png')
# ----------------------------

random.seed(SEED)

def find_class_parent(root):
    """
    Return the path whose immediate subdirectories are class folders containing images.
    Handles nested cases like /content/dataset/seg_train/seg_train.
    """
    candidates = [root] + [os.path.join(root, d) for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]
    for cand in candidates:
        try:
            subdirs = [d for d in os.listdir(cand) if os.path.isdir(os.path.join(cand, d))]
        except Exception:
            continue
        if not subdirs:
            continue
        for sd in subdirs:
            sd_path = os.path.join(cand, sd)
            if any(f.lower().endswith(IMG_EXTS) for f in os.listdir(sd_path)):
                return cand
    raise FileNotFoundError(f"No class-folder structure found under {root}. Run `!ls -R {root}` to inspect.")

# 1️⃣ Auto-find DATA_SRC
try:
    DATA_SRC = find_class_parent(UNZIP_ROOT)
    print("✅ Using DATA_SRC =", DATA_SRC)
except FileNotFoundError as e:
    print("ERROR - dataset not found:", e)
    print("\nRecursive listing of", UNZIP_ROOT, ":")
    !ls -R {UNZIP_ROOT}
    raise

# 2️⃣ List class folders
classes = sorted([d for d in os.listdir(DATA_SRC) if os.path.isdir(os.path.join(DATA_SRC, d))])
print("📂 Found classes:", classes)

# 3️⃣ Skip splitting if already done
if os.path.isdir(os.path.join(DATA_DIR, 'train')):
    print(f"✅ Split already exists at {DATA_DIR} - skipping split step.")
else:
    print("🚀 Creating new train/val/test splits...")
    os.makedirs(DATA_DIR, exist_ok=True)
    for cls in classes:
        cls_dir = os.path.join(DATA_SRC, cls)
        files = [f for f in glob.glob(os.path.join(cls_dir, '*')) if f.lower().endswith(IMG_EXTS)]
        if len(files) == 0:
            print(f"⚠️ No images found for class {cls} in {cls_dir}, skipping.")
            continue
        train_files, rest = train_test_split(files, test_size=0.2, random_state=SEED)
        val_files, test_files = train_test_split(rest, test_size=0.5, random_state=SEED)
        for split_name, fset in [('train', train_files), ('val', val_files), ('test', test_files)]:
            out_dir = os.path.join(DATA_DIR, split_name, cls)
            os.makedirs(out_dir, exist_ok=True)
            for src in fset:
                dst = os.path.join(out_dir, os.path.basename(src))
                shutil.copy(src, dst)
    print("✅ Splitting completed successfully!")

# 4️⃣ Summary counts
for split in ['train', 'val', 'test']:
    split_path = os.path.join(DATA_DIR, split)
    if os.path.isdir(split_path):
        count = sum(len(files) for _, _, files in os.walk(split_path))
        print(f"{split} : {count} images")
    else:
        print(f"{split} : (not created)")

# 5️⃣ Show folder tree top level
print("\n📁 Top-level of DATA_DIR:")
!ls -l {DATA_DIR}

!ls -R /content/dataset | head -n 40

from google.colab import files
uploaded = files.upload()  # Choose your ZIP file (e.g. archive.zip)

!ls /content

!mkdir -p /content/dataset
!unzip -q "/content/archive (6).zip" -d /content/dataset
!ls /content/dataset

!ls -R /content/dataset | head -n 50

# move the inner seg_train up one level
!mv /content/dataset/seg_train/seg_train /content/dataset/seg_train_fixed
!rm -rf /content/dataset/seg_train
!mv /content/dataset/seg_train_fixed /content/dataset/seg_train

# verify
!ls /content/dataset/seg_train

DATA_SRC = '/content/dataset/seg_train'

import os
print(os.listdir(DATA_SRC))

# Smart auto-check + fix + split cell
import os, glob, shutil, random
from sklearn.model_selection import train_test_split

random_seed = 42
SRC_ROOT = '/content/dataset'          # where you unzipped archive
OUT_ROOT = '/content/dataset_split'    # where split dataset will be created
IMG_EXTS = ('.jpg', '.jpeg', '.png')

def is_class_folder(path):
    # Heuristic: folder contains image files or subfolders that contain images
    try:
        entries = os.listdir(path)
    except Exception:
        return False
    # if many image files directly inside, treat as images folder (not class folder)
    img_files = [f for f in entries if f.lower().endswith(IMG_EXTS)]
    if img_files:
        return False
    # if subfolders exist and those subfolders contain images, this is parent-of-classes
    subdirs = [d for d in entries if os.path.isdir(os.path.join(path, d))]
    if not subdirs:
        return False
    for sd in subdirs:
        sd_path = os.path.join(path, sd)
        # check if subdir contains images
        for root,_,files in os.walk(sd_path):
            if any(f.lower().endswith(IMG_EXTS) for f in files):
                return True
    return False

# Step A: find candidate directories that contain classes
candidates = []
for root, dirs, files in os.walk(SRC_ROOT):
    # only immediate children level considered
    if root.count(os.sep) - SRC_ROOT.count(os.sep) > 2:
        # skip deep recursion to speed up
        continue
    if is_class_folder(root):
        candidates.append(root)

# If none found, try simpler heuristics
if not candidates:
    # check immediate children of SRC_ROOT
    for name in os.listdir(SRC_ROOT):
        p = os.path.join(SRC_ROOT, name)
        if os.path.isdir(p):
            # if that dir contains class-named subfolders (folders that contain images), pick it
            subdirs = [d for d in os.listdir(p) if os.path.isdir(os.path.join(p, d))]
            if any(len(glob.glob(os.path.join(p, sd, '*'+ext)))>0 for sd in subdirs for ext in IMG_EXTS):
                candidates.append(p)
    # fallback: if SRC_ROOT itself contains class folders directly (folders with images)
    if not candidates:
        subdirs = [d for d in os.listdir(SRC_ROOT) if os.path.isdir(os.path.join(SRC_ROOT, d))]
        if any(len(glob.glob(os.path.join(SRC_ROOT, sd, '*'+ext)))>0 for sd in subdirs for ext in IMG_EXTS):
            candidates.append(SRC_ROOT)

if not candidates:
    print("❌ Could not find image class folders under", SRC_ROOT)
    print("Run `!ls -R /content/dataset` to inspect manually or upload/unzip dataset into /content/dataset.")
    raise SystemExit

# If multiple candidates, pick the one with most image files
best = max(candidates, key=lambda p: sum(len(glob.glob(os.path.join(p, d, '*'+ext))) for d in os.listdir(p) if os.path.isdir(os.path.join(p,d)) for ext in IMG_EXTS))
print("✅ Detected class-parent folder:", best)

# If best is nested like /content/dataset/seg_train/seg_train, ensure we use inner that contains classes
# best already points to folder whose subdirs contain image files.

DATA_SRC = best
print("Using DATA_SRC =", DATA_SRC)
print("Classes found:")
classes = sorted([d for d in os.listdir(DATA_SRC) if os.path.isdir(os.path.join(DATA_SRC, d))])
print(classes)

# Create split (80/10/10)
os.makedirs(OUT_ROOT, exist_ok=True)
train_count = val_count = test_count = 0

for cls in classes:
    cls_dir = os.path.join(DATA_SRC, cls)
    img_files = [f for f in glob.glob(os.path.join(cls_dir, '*')) if f.lower().endswith(IMG_EXTS)]
    img_files = sorted(img_files)
    if len(img_files) == 0:
        print(f"⚠️ No images found for class {cls}, skipping.")
        continue
    train_files, rest = train_test_split(img_files, test_size=0.2, random_state=random_seed)
    val_files, test_files = train_test_split(rest, test_size=0.5, random_state=random_seed)
    for split_name, fileset in [('train', train_files), ('val', val_files), ('test', test_files)]:
        out_dir = os.path.join(OUT_ROOT, split_name, cls)
        os.makedirs(out_dir, exist_ok=True)
        for f in fileset:
            shutil.copy(f, os.path.join(out_dir, os.path.basename(f)))
    train_count += len(train_files)
    val_count += len(val_files)
    test_count += len(test_files)

print(f"\n🎯 Split complete. Totals -> train: {train_count}, val: {val_count}, test: {test_count}")
print("\nSample directory tree (top levels):")
!ls -R /content/dataset_split | sed -n '1,200p'

DATA_DIR = '/content/dataset_split'

# Verify dataset_split structure & counts
import os, glob
DATA_DIR = '/content/dataset_split'
for split in ['train','val','test']:
    root = os.path.join(DATA_DIR, split)
    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))])
    total = sum(len(glob.glob(os.path.join(root,c,'*'))) for c in classes)
    print(f"{split}: {total} images, classes: {classes[:10]}")

# Transforms and dataloaders
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
IMG_SIZE = 224
BATCH = 32         # reduce to 16 if OOM on T4
NUM_WORKERS = 2

train_tfm = transforms.Compose([
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7,1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2,0.2,0.2,0.05),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
val_tfm = transforms.Compose([
    transforms.Resize(int(IMG_SIZE*1.15)),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

train_ds = datasets.ImageFolder(os.path.join(DATA_DIR,'train'), transform=train_tfm)
val_ds   = datasets.ImageFolder(os.path.join(DATA_DIR,'val'),   transform=val_tfm)
test_ds  = datasets.ImageFolder(os.path.join(DATA_DIR,'test'),  transform=val_tfm)

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

class_names = train_ds.classes
print("Classes:", class_names)
print("Train/Val/Test sizes:", len(train_ds), len(val_ds), len(test_ds))

# Two-stage train: EfficientNet-B0 via timm
import torch, timm, copy, time, numpy as np
import torch.nn as nn, torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True, num_classes=len(class_names))
model.to(DEVICE)
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# Stage 1: head-only
for name, p in model.named_parameters():
    p.requires_grad = False
for name, p in model.named_parameters():
    if 'classifier' in name or 'head' in name or 'fc' in name:
        p.requires_grad = True

opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)
sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=3)
E1 = 6
best_val_acc = 0.0
best_state = None
SAVE_PATH = '/content/best_model.pth'

def run_epoch(loader, train=True):
    if train: model.train()
    else: model.eval()
    running_loss = 0.0; preds=[]; targs=[]
    for imgs, labels in loader:
        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
        if train: opt.zero_grad()
        with torch.set_grad_enabled(train):
            logits = model(imgs)
            loss = criterion(logits, labels)
            if train:
                loss.backward(); opt.step()
        running_loss += loss.item()*imgs.size(0)
        preds.extend(torch.argmax(logits,1).cpu().tolist())
        targs.extend(labels.cpu().tolist())
    avg_loss = running_loss/len(loader.dataset)
    acc = (np.array(preds)==np.array(targs)).mean()
    return avg_loss, acc

for epoch in range(1, E1+1):
    t0=time.time()
    tr_loss, tr_acc = run_epoch(train_loader, train=True)
    val_loss, val_acc = run_epoch(val_loader, train=False)
    sched.step(val_loss)
    print(f"[Stage1] Epoch {epoch}/{E1} tr_acc={tr_acc:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s")
    if val_acc>best_val_acc:
        best_val_acc=val_acc; best_state=copy.deepcopy(model.state_dict()); torch.save({'model_state':best_state,'class_names':class_names}, SAVE_PATH)

# Stage 2: unfreeze all & fine tune
for p in model.parameters(): p.requires_grad = True
opt = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)
sched2 = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20)
E2 = 25
patience = 7; noimp=0

for epoch in range(1, E2+1):
    t0=time.time()
    tr_loss, tr_acc = run_epoch(train_loader, train=True)
    val_loss, val_acc = run_epoch(val_loader, train=False)
    sched2.step()
    print(f"[Stage2] Epoch {epoch}/{E2} tr_acc={tr_acc:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s")
    if val_acc>best_val_acc:
        best_val_acc=val_acc; best_state=copy.deepcopy(model.state_dict()); torch.save({'model_state':best_state,'class_names':class_names}, SAVE_PATH); noimp=0
    else:
        noimp+=1
    if noimp>=patience:
        print("Early stopping")
        break

print("Best val acc:", best_val_acc)

# load best and evaluate
ckpt = torch.load('/content/best_model.pth', map_location=DEVICE)
model.load_state_dict(ckpt['model_state'])
model.eval()

all_preds=[]; all_targs=[]
with torch.no_grad():
    for imgs, labels in test_loader:
        imgs = imgs.to(DEVICE)
        logits = model(imgs)
        all_preds += logits.argmax(1).cpu().tolist()
        all_targs += labels.cpu().tolist()

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(all_targs, all_preds, target_names=class_names))
print("Confusion matrix:")
print(confusion_matrix(all_targs, all_preds))

# Simple TTA: center crop + horizontal flip average
import torch.nn.functional as F
from torchvision.transforms import functional as TF
from PIL import Image
base_tfm = val_tfm  # reuse transform

def tta_predict_image(path, model):
    imgs = []
    img = Image.open(path).convert('RGB')
    x1 = base_tfm(img).unsqueeze(0).to(DEVICE)
    x2 = TF.hflip(base_tfm(img)).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        p = F.softmax(model(x1), dim=1) + F.softmax(model(x2), dim=1)
        p = p/2.0
        idx = int(torch.argmax(p, dim=1).cpu().item())
        return class_names[idx], float(torch.max(p).cpu().item())

from google.colab import files
files.download('/content/best_model.pth')

# run once
!pip install -q timm==0.9.2 seaborn

import os, torch
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
import timm

# CONFIG - change only if your folders differ
DATA_DIR = '/content/dataset_split'   # the split created earlier
CHECKPOINT = '/content/best_model.pth'
IMG_SIZE = 224
BATCH = 32
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# load checkpoint
ckpt = torch.load(CHECKPOINT, map_location=DEVICE)
class_names = ckpt.get('class_names', None)
if class_names is None:
    # fallback: try reading from train folder
    class_names = sorted([d for d in os.listdir(os.path.join(DATA_DIR,'train')) if os.path.isdir(os.path.join(DATA_DIR,'train',d))])
print("Classes:", class_names)

# build model (same backbone as training)
model = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=len(class_names))
model.load_state_dict(ckpt['model_state'])
model.to(DEVICE)
model.eval()

# transforms (same as val transforms used in training)
val_tfm = transforms.Compose([
    transforms.Resize(int(IMG_SIZE*1.15)),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

test_ds = datasets.ImageFolder(os.path.join(DATA_DIR,'test'), transform=val_tfm)
test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)

print("Test dataset size:", len(test_ds))

# compute predictions on test set
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

all_preds = []
all_targs = []
with torch.no_grad():
    for imgs, labels in test_loader:
        imgs = imgs.to(DEVICE)
        logits = model(imgs)
        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()
        all_preds.extend(preds)
        all_targs.extend(labels.numpy().tolist())

print("Accuracy:", accuracy_score(all_targs, all_preds))
print("\nClassification report:\n")
print(classification_report(all_targs, all_preds, target_names=class_names))

# confusion matrix heatmap
import matplotlib.pyplot as plt
import seaborn as sns
cm = confusion_matrix(all_targs, all_preds)
plt.figure(figsize=(9,7))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')
plt.show()

import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import cv2

# Utility: find last Conv2d module name & module
def find_last_conv_module(model):
    last_name, last_mod = None, None
    for name, m in model.named_modules():
        if isinstance(m, torch.nn.Conv2d):
            last_name, last_mod = name, m
    return last_name, last_mod

last_conv_name, last_conv_mod = find_last_conv_module(model)
print("Last conv layer:", last_conv_name)

# Grad-CAM helper class
class GradCAM:
    def __init__(self, model, target_layer_name):
        self.model = model
        self.model.eval()
        self.device = next(model.parameters()).device
        self.activation = None
        self.gradients = None
        # register hooks on target layer
        for name, module in self.model.named_modules():
            if name == target_layer_name:
                module.register_forward_hook(self.save_activation)
                module.register_full_backward_hook(self.save_gradient)
                break
        if target_layer_name is None:
            raise ValueError("target_layer_name not found in model")

    def save_activation(self, module, input, output):
        # output: (N, C, H, W)
        self.activation = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        # grad_output is a tuple
        self.gradients = grad_output[0].detach()

    def compute(self, input_tensor, class_idx=None):
        """
        input_tensor: (1,3,H,W) preprocessed and on device
        class_idx: target class index to explain. If None, uses top1 prediction.
        returns heatmap (H,W) normalized between 0-1
        """
        self.model.zero_grad()
        output = self.model(input_tensor)  # forward
        if class_idx is None:
            class_idx = int(output.argmax(dim=1).item())
        loss = output[0, class_idx]
        loss.backward(retain_graph=True)

        # grads: (N, C, H, W); acts: (N, C, H, W)
        grads = self.gradients[0]      # (C, H, W)
        acts = self.activation[0]      # (C, H, W)

        # global average pooling of grads -> weights
        weights = grads.mean(dim=(1,2))  # (C,)

        # weighted sum of activation maps
        cam = (weights[:, None, None] * acts).sum(dim=0)  # (H, W)
        cam = F.relu(cam)
        cam_np = cam.cpu().numpy()
        # normalize
        cam_np -= cam_np.min()
        if cam_np.max() != 0:
            cam_np /= cam_np.max()
        # resize to input size (H_in, W_in)
        cam_np = cv2.resize(cam_np, (input_tensor.shape[3], input_tensor.shape[2]))
        return cam_np, class_idx

# overlay function
def overlay_heatmap_on_image(img_pil, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):
    """
    img_pil: PIL.Image (size W,H)
    heatmap: numpy array (H,W) normalized 0-1
    """
    img = np.array(img_pil)[:,:,::-1]  # RGB->BGR for cv2
    hmap = np.uint8(255 * heatmap)
    hmap = cv2.applyColorMap(hmap, colormap)
    hmap = cv2.resize(hmap, (img.shape[1], img.shape[0]))
    overlay = cv2.addWeighted(hmap, alpha, img, 1-alpha, 0)
    overlay = overlay[:,:,::-1]  # BGR->RGB
    return overlay

# create GradCAM object
gradcam = GradCAM(model, last_conv_name)
print("GradCAM ready.")

import random
from torchvision.transforms import functional as TF
from PIL import Image

# Pick some test image paths (randomly sample a few)
test_root = os.path.join(DATA_DIR, 'test')
all_image_paths = []
for cls in class_names:
    p = os.path.join(test_root, cls)
    if os.path.isdir(p):
        all_image_paths += [os.path.join(p, f) for f in os.listdir(p) if f.lower().endswith(('.jpg','.png','.jpeg'))]

random.seed(42)
sample_paths = random.sample(all_image_paths, min(12, len(all_image_paths)))
print("Sample images:", len(sample_paths))

def preprocess_for_model(img_path):
    img = Image.open(img_path).convert('RGB')
    x = val_tfm(img).unsqueeze(0).to(DEVICE)  # shape (1,3,H,W)
    return img, x

# Display function
def show_gradcam_on_path(path):
    orig_img, inp = preprocess_for_model(path)
    heatmap, class_idx = gradcam.compute(inp, class_idx=None)  # top1 by default
    pred_label = class_names[class_idx]
    overlay = overlay_heatmap_on_image(orig_img, heatmap)
    # plot
    plt.figure(figsize=(8,4))
    plt.suptitle(f"Predicted: {pred_label}")
    plt.subplot(1,2,1)
    plt.imshow(orig_img); plt.axis('off'); plt.title('Original')
    plt.subplot(1,2,2)
    plt.imshow(overlay); plt.axis('off'); plt.title('Grad-CAM overlay')
    plt.show()

# Run for sample images
for p in sample_paths:
    show_gradcam_on_path(p)

OUT_DIR = '/content/gradcam_results'
os.makedirs(OUT_DIR, exist_ok=True)
for i,p in enumerate(sample_paths):
    orig_img, inp = preprocess_for_model(p)
    heatmap, class_idx = gradcam.compute(inp, class_idx=None)
    overlay = overlay_heatmap_on_image(orig_img, heatmap)
    out_path = os.path.join(OUT_DIR, f"gradcam_{i}_{class_names[class_idx]}.png")
    from PIL import Image
    Image.fromarray((overlay).astype('uint8')).save(out_path)
print("Saved Grad-CAM images to", OUT_DIR)

#phase 2

# Phase-2 Report generator (Colab)
# - recomputes test metrics & confusion matrix
# - regenerates Grad-CAM images (a few samples)
# - creates a PDF report at /content/scene_recog_report.pdf
# REQUIREMENTS: timm, reportlab, matplotlib, seaborn, opencv-python
!pip install -q timm==0.9.2 reportlab seaborn opencv-python-headless

import os, torch, timm, copy, random
import numpy as np
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
import cv2
from PIL import Image
import torch.nn.functional as F

# CONFIG - adjust if your paths differ
DATA_DIR = '/content/dataset_split'   # train/val/test created earlier
CHECKPOINT = '/content/best_model.pth'
IMG_SIZE = 224
BATCH = 32
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
OUT_DIR = '/content/report_assets'
os.makedirs(OUT_DIR, exist_ok=True)

# Load model + classes
ckpt = torch.load(CHECKPOINT, map_location=DEVICE)
class_names = ckpt.get('class_names', None)
if class_names is None:
    class_names = sorted([d for d in os.listdir(os.path.join(DATA_DIR,'train')) if os.path.isdir(os.path.join(DATA_DIR,'train',d))])
model = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=len(class_names))
model.load_state_dict(ckpt['model_state'])
model.to(DEVICE)
model.eval()

# transforms
val_tfm = transforms.Compose([
    transforms.Resize(int(IMG_SIZE*1.15)),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

test_ds = datasets.ImageFolder(os.path.join(DATA_DIR,'test'), transform=val_tfm)
test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)

# 1) Evaluate & save classification report + confusion matrix image
all_preds, all_targs = [], []
with torch.no_grad():
    for imgs, labels in test_loader:
        imgs = imgs.to(DEVICE)
        logits = model(imgs)
        preds = logits.argmax(1).cpu().numpy().tolist()
        all_preds.extend(preds)
        all_targs.extend(labels.cpu().numpy().tolist())

acc = accuracy_score(all_targs, all_preds)
cls_report = classification_report(all_targs, all_preds, target_names=class_names, digits=4)
cm = confusion_matrix(all_targs, all_preds)

# save confusion matrix figure
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title(f'Confusion Matrix (acc={acc:.4f})')
cm_path = os.path.join(OUT_DIR, 'confusion_matrix.png')
plt.tight_layout(); plt.savefig(cm_path, dpi=200); plt.close()

# 2) Grad-CAM: find last conv automatically and make a few overlays
def find_last_conv_module(model):
    last_name, last_mod = None, None
    for name, m in model.named_modules():
        if isinstance(m, torch.nn.Conv2d):
            last_name, last_mod = name, m
    return last_name

last_conv_name = find_last_conv_module(model)
print("Detected last conv layer:", last_conv_name)

# register hooks
activation = None; gradients = None
def forward_hook(module, inp, out):
    global activation; activation = out.detach()
def backward_hook(module, grad_in, grad_out):
    global gradients; gradients = grad_out[0].detach()

# attach hooks to the module object
for name, module in model.named_modules():
    if name == last_conv_name:
        module.register_forward_hook(forward_hook)
        module.register_full_backward_hook(backward_hook)
        break

# helper to compute grad-cam heatmap
def compute_gradcam_for_tensor(x_tensor, class_idx=None):
    global activation, gradients
    activation = None; gradients = None
    model.zero_grad()
    out = model(x_tensor)
    if class_idx is None:
        class_idx = int(out.argmax(dim=1).item())
    loss = out[0, class_idx]
    loss.backward(retain_graph=True)
    grads = gradients[0]   # C,H,W
    acts = activation[0]   # C,H,W
    weights = grads.mean(dim=(1,2))  # C
    cam = (weights[:,None,None] * acts).sum(dim=0)  # H,W
    cam = F.relu(cam).cpu().numpy()
    cam -= cam.min()
    if cam.max()!=0:
        cam /= cam.max()
    cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))
    return cam_resized, class_idx

# pick sample images from test set (up to 8)
test_root = os.path.join(DATA_DIR, 'test')
all_paths = []
for cls in class_names:
    p = os.path.join(test_root, cls)
    if os.path.isdir(p):
        for f in os.listdir(p):
            if f.lower().endswith(('.jpg','.png','.jpeg')):
                all_paths.append(os.path.join(p, f))
random.seed(42)
sample_paths = random.sample(all_paths, min(8, len(all_paths)))

gradcam_paths = []
for i, pth in enumerate(sample_paths):
    img_pil = Image.open(pth).convert('RGB')
    x = val_tfm(img_pil).unsqueeze(0).to(DEVICE)
    cam, cls_idx = compute_gradcam_for_tensor(x, class_idx=None)
    pred_label = class_names[cls_idx]
    # overlay
    img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    heatmap = np.uint8(255 * cam)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    heatmap = cv2.resize(heatmap, (img_cv.shape[1], img_cv.shape[0]))
    overlay = cv2.addWeighted(heatmap, 0.5, img_cv, 0.5, 0)
    overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)
    out_path = os.path.join(OUT_DIR, f'gradcam_{i}_{pred_label}.png')
    Image.fromarray(overlay).save(out_path)
    gradcam_paths.append(out_path)

# 3) Build PDF with reportlab
pdf_path = '/content/scene_recog_report.pdf'
c = canvas.Canvas(pdf_path, pagesize=A4)
W, H = A4

# Title
c.setFont("Helvetica-Bold", 18)
c.drawString(40, H-60, "Deep Learning-based Scene Recognition — Report")
c.setFont("Helvetica", 10)
c.drawString(40, H-80, f"Model: EfficientNet-B0 (timm) | Checkpoint: {os.path.basename(CHECKPOINT)}")
c.drawString(40, H-95, f"Dataset split: {DATA_DIR} | Test size: {len(test_ds)} images")
c.drawString(40, H-110, f"Overall test accuracy: {acc:.4f}")

# Add classification report text (wrap)
text_y = H-130
c.setFont("Helvetica-Bold", 12)
c.drawString(40, text_y, "Classification Report (precision / recall / f1):")
text_y -= 14
c.setFont("Helvetica", 8)
# write small block of the classification report
for line in cls_report.splitlines():
    if text_y < 100:
        c.showPage(); text_y = H-60
    c.drawString(40, text_y, line[:150])
    text_y -= 10

# Add confusion matrix image
c.showPage()
c.setFont("Helvetica-Bold", 14)
c.drawString(40, H-60, "Confusion Matrix")
img = ImageReader(cm_path)
c.drawImage(img, 40, H-420, width=520, height=350, preserveAspectRatio=True)

# Add Grad-CAM images (grid)
c.showPage()
c.setFont("Helvetica-Bold", 14)
c.drawString(40, H-60, "Grad-CAM Visualizations (sample images)")
x = 40; y = H-120
max_h = 200
for i, gp in enumerate(gradcam_paths):
    if i>0 and i%2==0:
        y -= (max_h+40); x = 40
        if y < 120:
            c.showPage(); x=40; y=H-120
    img = ImageReader(gp)
    c.drawImage(img, x, y-max_h, width=260, height=max_h, preserveAspectRatio=True)
    c.drawString(x, y-max_h-12, os.path.basename(gp))
    x += 280

c.showPage()
c.setFont("Helvetica-Bold", 12)
c.drawString(40, H-60, "Notes & Next Steps")
c.setFont("Helvetica", 10)
c.drawString(40, H-80, "- Consider TTA and small ensemble for +1~2% boost if needed.")
c.drawString(40, H-95, "- For deployment: quantize with torch.quantization or export to ONNX/TorchScript.")
c.drawString(40, H-110, "- Create Streamlit demo to show predictions + Grad-CAM (app provided).")

c.save()
print("Report saved to", pdf_path)

from google.colab import files
files.download('/content/scene_recog_report.pdf')

# ✅ SHOW current content
!echo "==== /content listing ===="
!ls -la /content

# ✅ CREATE project folder if missing
!mkdir -p /content/scene-recognition-system
!echo "Created /content/scene-recognition-system (if not present)"

# ✅ MOVE existing files into project folder (no overwrite)
!for f in best_model.pth train.ipynb scene_recog.ipynb scene_recog_report.pdf README.md requirements.txt .gitignore app.py; do \
    if [ -e "/content/$f" ]; then \
        mv -n "/content/$f" /content/scene-recognition-system/ && echo "Moved $f -> scene-recognition-system/"; \
    fi; \
done

# ✅ SHOW final folder structure
!echo "==== /content/scene-recognition-system listing ===="
!ls -la /content/scene-recognition-system || true

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/scene-recognition-system

import os
os.chdir("/content/scene-recognition-system")
print("Now inside:", os.getcwd())

!ls -lh /content

import nbformat

input_path = "/content/train.ipynb"              # ← correct file name
output_path = "/content/train_clean.ipynb"       # ← cleaned version

with open(input_path) as f:
    nb = nbformat.read(f, as_version=4)

# remove invalid metadata (fixes GitHub "Invalid Notebook" error)
if "widgets" in nb["metadata"]:
    del nb["metadata"]["widgets"]

# write cleaned notebook
with open(output_path, "w") as f:
    nbformat.write(nb, f)

print("✅ Cleaned notebook saved as:", output_path)

!ls -lh /content | grep clean

!mv /content/train_clean.ipynb /content/scene-recognition-system/train.ipynb
!ls -lh /content/scene-recognition-system